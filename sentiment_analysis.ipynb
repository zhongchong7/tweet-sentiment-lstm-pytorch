{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7c964349",
      "metadata": {},
      "source": [
        "Import Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06157733",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, classification_report\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75e4b624",
      "metadata": {},
      "source": [
        "Data loading: csv file is downloaded from kaggle "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41fca203",
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv('Tweets.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e9c6464",
      "metadata": {},
      "source": [
        "Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "974c31b5",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Ensure this location is on NLTK's search path\n",
        "nltk_data_dir = Path.home() / \"nltk_data\"\n",
        "if str(nltk_data_dir) not in nltk.data.path:\n",
        "    nltk.data.path.append(str(nltk_data_dir))\n",
        "\n",
        "# Tokenizer + stopwords resources\n",
        "nltk.download(\"punkt\", download_dir=str(nltk_data_dir))\n",
        "# Some NLTK versions require this for word_tokenize\n",
        "nltk.download(\"punkt_tab\", download_dir=str(nltk_data_dir))\n",
        "nltk.download(\"stopwords\", download_dir=str(nltk_data_dir))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3b8297b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# convert text to lowercase (vectorized)\n",
        "df[\"text\"] = df[\"text\"].astype(str).str.lower()\n",
        "\n",
        "# Tokenization and Stopwords Removal\n",
        "# Tokenize the text\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "df[\"tokens\"] = df[\"text\"].apply(word_tokenize)\n",
        "\n",
        "# Remove stopwords (compute once; use a set for faster membership tests)\n",
        "stopword_set = set(nltk.corpus.stopwords.words(\"english\"))\n",
        "df[\"tokens\"] = df[\"tokens\"].apply(lambda toks: [w for w in toks if w not in stopword_set])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af7641b6",
      "metadata": {},
      "source": [
        "Split the dataset into training and testing sets to evaluate the performance of the sentiment analysis model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3628e7f2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use tokenized text (list[str]) as the model input\n",
        "X = df[\"tokens\"]\n",
        "y = df[\"sentiment\"]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X,\n",
        "    y,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y,\n",
        ")\n",
        "\n",
        "# Encode sentiment labels to integer class IDs for PyTorch\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_enc = label_encoder.fit_transform(y_train)\n",
        "y_test_enc = label_encoder.transform(y_test)\n",
        "num_classes = len(label_encoder.classes_)\n",
        "print(\"Classes:\", list(label_encoder.classes_))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d04e19bd",
      "metadata": {},
      "source": [
        "Convert tokenized text into integer IDs (a vocabulary). Then an `nn.Embedding` layer will map those IDs to dense word vectors for the RNN to learn from."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e984a23",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build a vocabulary on the TRAIN split only (avoid leakage)\n",
        "PAD_TOKEN = \"<pad>\"\n",
        "UNK_TOKEN = \"<unk>\"\n",
        "PAD_ID = 0\n",
        "UNK_ID = 1\n",
        "\n",
        "max_vocab_size = 20000\n",
        "min_freq = 2\n",
        "max_len = 60\n",
        "\n",
        "counter = Counter()\n",
        "for toks in X_train:\n",
        "    counter.update(toks)\n",
        "\n",
        "vocab_tokens = [tok for tok, freq in counter.most_common(max_vocab_size) if freq >= min_freq]\n",
        "itos = [PAD_TOKEN, UNK_TOKEN] + vocab_tokens\n",
        "stoi = {tok: i for i, tok in enumerate(itos)}\n",
        "vocab_size = len(itos)\n",
        "\n",
        "print(\"Vocab size:\", vocab_size)\n",
        "\n",
        "\n",
        "def numericalize(tokens):\n",
        "    return [stoi.get(t, UNK_ID) for t in tokens]\n",
        "\n",
        "\n",
        "# Keep variable lengths; pad per-batch (pairs with pack_padded_sequence)\n",
        "X_train_num = [numericalize(toks) for toks in X_train]\n",
        "X_test_num = [numericalize(toks) for toks in X_test]\n",
        "\n",
        "y_train_ids = np.array(y_train_enc, dtype=np.int64)\n",
        "y_test_ids = np.array(y_test_enc, dtype=np.int64)\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "def clip_len(x):\n",
        "    x = x[:max_len]\n",
        "    return x if len(x) > 0 else [UNK_ID]\n",
        "\n",
        "\n",
        "class SeqDataset(Dataset):\n",
        "    def __init__(self, X_num, y_ids):\n",
        "        self.X = [clip_len(x) for x in X_num]\n",
        "        self.y = y_ids\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.X[idx]\n",
        "        return torch.tensor(x, dtype=torch.long), int(self.y[idx]), len(x)\n",
        "\n",
        "\n",
        "def collate_batch(batch):\n",
        "    xs, ys, lens = zip(*batch)\n",
        "\n",
        "    # sort by length descending (required for pack_padded_sequence)\n",
        "    order = np.argsort(lens)[::-1]\n",
        "    xs = [xs[i] for i in order]\n",
        "    ys = torch.tensor([ys[i] for i in order], dtype=torch.long)\n",
        "    lens = torch.tensor([lens[i] for i in order], dtype=torch.long)\n",
        "\n",
        "    xpad = nn.utils.rnn.pad_sequence(xs, batch_first=True, padding_value=PAD_ID)\n",
        "    return xpad, ys, lens\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "train_ds = SeqDataset(X_train_num, y_train_ids)\n",
        "test_ds = SeqDataset(X_test_num, y_test_ids)\n",
        "\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
        "test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, collate_fn=collate_batch)\n",
        "\n",
        "print(\"Device:\", device)\n",
        "print(\"Train batches:\", len(train_loader), \"Test batches:\", len(test_loader))\n",
        "\n",
        "# Class weights to reduce majority-class collapse\n",
        "class_counts = np.bincount(y_train_ids, minlength=num_classes)\n",
        "class_weights = (class_counts.sum() / (num_classes * np.maximum(class_counts, 1))).astype(np.float32)\n",
        "class_weights_t = torch.tensor(class_weights, dtype=torch.float32, device=device)\n",
        "print(\"Class counts:\", class_counts, \"weights:\", class_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41d220e2",
      "metadata": {},
      "source": [
        "Build and Train a Sentiment Analysis Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4e70e57",
      "metadata": {},
      "outputs": [],
      "source": [
        "# LSTM sentiment classifier (learned word embeddings)\n",
        "torch.manual_seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(42)\n",
        "\n",
        "\n",
        "class SentimentLSTM(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int,\n",
        "        embed_dim: int,\n",
        "        hidden_dim: int,\n",
        "        num_classes: int,\n",
        "        pad_idx: int = 0,\n",
        "        dropout: float = 0.3,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self, x, lengths):\n",
        "        # x: [batch, seq_len], lengths: [batch]\n",
        "        emb = self.embedding(x)  # [batch, seq_len, embed_dim]\n",
        "        packed = nn.utils.rnn.pack_padded_sequence(\n",
        "            emb,\n",
        "            lengths.cpu(),\n",
        "            batch_first=True,\n",
        "            enforce_sorted=True,\n",
        "        )\n",
        "        _, (h_n, _) = self.lstm(packed)\n",
        "        h_last = h_n[-1]  # [batch, hidden_dim]\n",
        "        h_last = self.dropout(h_last)\n",
        "        return self.fc(h_last)  # [batch, num_classes]\n",
        "\n",
        "\n",
        "model = SentimentLSTM(\n",
        "    vocab_size=vocab_size,\n",
        "    embed_dim=128,\n",
        "    hidden_dim=128,\n",
        "    num_classes=num_classes,\n",
        "    pad_idx=PAD_ID,\n",
        "    dropout=0.3,\n",
        ").to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights_t)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "\n",
        "def run_epoch(loader, train: bool):\n",
        "    if train:\n",
        "        model.train()\n",
        "    else:\n",
        "        model.eval()\n",
        "\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for xb, yb, lens in loader:\n",
        "        xb = xb.to(device)\n",
        "        yb = yb.to(device)\n",
        "        lens = lens.to(device)\n",
        "\n",
        "        if train:\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        with torch.set_grad_enabled(train):\n",
        "            logits = model(xb, lens)\n",
        "            loss = criterion(logits, yb)\n",
        "            if train:\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * xb.size(0)\n",
        "        preds = logits.argmax(dim=1)\n",
        "        correct += (preds == yb).sum().item()\n",
        "        total += xb.size(0)\n",
        "\n",
        "    return total_loss / total, correct / total\n",
        "\n",
        "\n",
        "epochs = 8\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train_loss, train_acc = run_epoch(train_loader, train=True)\n",
        "    test_loss, test_acc = run_epoch(test_loader, train=False)\n",
        "    print(\n",
        "        f\"Epoch {epoch}/{epochs} | \"\n",
        "        f\"train loss {train_loss:.4f} acc {train_acc:.4f} | \"\n",
        "        f\"test loss {test_loss:.4f} acc {test_acc:.4f}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ec640db",
      "metadata": {},
      "source": [
        "Use the trained model to make predictions on the test data and evaluate its performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a67f020a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate on the test split\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_true = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for xb, yb, lens in test_loader:\n",
        "        xb = xb.to(device)\n",
        "        lens = lens.to(device)\n",
        "        logits = model(xb, lens)\n",
        "        preds = logits.argmax(dim=1).cpu().numpy()\n",
        "        all_preds.extend(preds.tolist())\n",
        "        all_true.extend(yb.numpy().tolist())\n",
        "\n",
        "all_preds = np.array(all_preds)\n",
        "all_true = np.array(all_true)\n",
        "\n",
        "print(\"Accuracy Score:\", accuracy_score(all_true, all_preds))\n",
        "print(\"Classification Report:\")\n",
        "print(\n",
        "    classification_report(\n",
        "        label_encoder.inverse_transform(all_true),\n",
        "        label_encoder.inverse_transform(all_preds),\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75deb2b8",
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# WordCloud is optional; fall back gracefully if not installed.\n",
        "try:\n",
        "    from wordcloud import WordCloud\n",
        "except ModuleNotFoundError:\n",
        "    WordCloud = None\n",
        "\n",
        "\n",
        "sent = \"positive\"\n",
        "texts = df[df[\"sentiment\"] == sent][\"text\"].astype(str)\n",
        "joined = \" \".join(texts)\n",
        "\n",
        "if WordCloud is None:\n",
        "    print(\"Optional dependency missing: wordcloud\")\n",
        "    print(\"Install with: pip install wordcloud\")\n",
        "\n",
        "    # Simple fallback: show top words as a bar chart\n",
        "    from collections import Counter\n",
        "\n",
        "    words = joined.lower().split()\n",
        "    top = Counter(words).most_common(20)\n",
        "    labels = [w for w, _ in top]\n",
        "    values = [c for _, c in top]\n",
        "\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.bar(labels, values)\n",
        "    plt.xticks(rotation=60, ha=\"right\")\n",
        "    plt.title(f\"Top words - {sent} sentiment\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    wc = WordCloud(width=800, height=400, background_color=\"white\").generate(joined)\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.imshow(wc, interpolation=\"bilinear\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(f\"Word Cloud - {sent.capitalize()} Sentiment\")\n",
        "    plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "py310",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
